\documentclass[a4paper,11pt]{article}
\usepackage{amsmath,amssymb}
\usepackage{amsthm}
\usepackage[english]{babel}
% \usepackage[latin1]{inputenc}
% \usepackage[T1]{fontenc}
% \usepackage{hyperref}
\usepackage[colorlinks = true, linkcolor=blue, citecolor= dgreen,urlcolor = Levanda]{hyperref}
% \usepackage[top = 3cm,bottom = 3cm,right = 3cm,left = 3cm]{geometry}
\usepackage[paperwidth=8.5in, paperheight=10.75in]{geometry}
\usepackage{tabularx}
\usepackage{xcolor}
\usepackage{lscape}
\usepackage{eulervm,mathtools}
\usepackage{caption}
\usepackage{here}
% \renewcommand{\fnum@figure}{Figure \thefigure}
% \captionsetup[figure]{name={Figure}}

\usepackage{background}
\usepackage{tabularx}
\usepackage{hyperref}
%\usepackage{pp4link}
\usepackage{mpmulti}
\usepackage{graphicx}
\graphicspath{{./figure/}}

\usepackage{subfig}

\usepackage[display]{texpower}
\usepackage{pause}
\usepackage{titlesec}
\usepackage{fancyhdr}
\usepackage{enumerate}

 \definecolor{Headcolor}{cmyk}{0,0,0,1}  %{0,0,0,0.65}  %
 \renewcommand\normalcolor{\color{Headcolor}}

 \definecolor{Textcolor}{cmyk}{0,0,0,1} % currently black (duh)
 \definecolor{Highlight}{cmyk}{0,0.89,0.94,0.1} % currently BrickRed
 \definecolor{Seagreen}{cmyk}{0.47,0,0.37,0.36} %0.26
 \definecolor{Beige}{cmyk}{0,0.02,0.20,0.06}
 \definecolor{LsteelB}{cmyk}{0.6, 0.3, 0, 0.2}
 \definecolor{Levanda}{cmyk}{.21, .37, 0, .53}
 \definecolor{Salm}{cmyk}{0,.4,0.5,0.2}
 \definecolor{dgrey}{cmyk}{0,0,0,0.995} %%{0,0,0,0.65}
 \definecolor{dgreen}{cmyk}{.90, 0, .7, .6}
 \definecolor{darkpastelblue}{rgb}{0.47, 0.62, 0.8}
 \definecolor{lightblue}{rgb}{0.68, 0.85, 0.9}
 \definecolor{lightskyblue}{rgb}{0.53, 0.81, 0.98}
 \definecolor{pastelgreen}{rgb}{0.47, 0.87, 0.47}

 \newcommand\beige{\color{Beige}}
 \newcommand\Leva{\color{Levanda}}
 \newcommand\Text{\color{Textcolor}}
 \newcommand\High{\color{Highlight}}
 \newcommand\SeaG{\color{Seagreen}}
 \newcommand\LstB{\color{LsteelB}}
 \newcommand\head{\color{Headcolor}}
 \newcommand\Salm{\color{Salm}}
 \newcommand\dgrey{\color{dgrey}}
 \newcommand\dgreen{\color{dgreen}}
 \newcommand\dpastelblue{\color{darkpastelblue}}
 \newcommand\lblue{\color{lightblue}}
 \newcommand\lightskyblue{\color{lightskyblue}}
 \newcommand\pastelgreen{\color{pastelgreen}}


%\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]
%\theoremstyle{plain}
\newtheorem{thrm}{Theorem}[section]
\newtheorem{prop}{Proposition}[section]
\newtheorem{ex}{Example}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{lemme}{Lemma}[section]
\newtheorem{coro}{Corollary}[section]

\newcommand{\R}{\mathbb{R}}
\newcommand{\Rplus}{\mathbb{R_+}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\calE}{\mathcal{E}}
\newcommand{\calR}{\mathcal{R}}
\newcommand{\calP}{\mathcal{P}}
\newcommand{\pr}{\mathbb{P}}
\newcommand{\intI}{\int_{0}^\infty}
\newcommand{\D}{\displaystyle}

\numberwithin{equation}{section}

%
\titleformat{\section}
    {\dpastelblue\Large\scshape\raggedright\vspace{1.75cm}}
    {\thesection}
    {1em}
    {}[{\titlerule[1.75pt]}]
%
\titleformat{\subsection}
    {\dpastelblue\large\scshape\raggedright\vspace{0cm}}
    {\thesubsection}
    {1em}
    {}[]
 \backgroundsetup{color=black,scale=3,contents={}} % remove DRAFT in red





\begin{document}

<<setup, include=FALSE>>=
	library(knitr)
	
	library(ggplot2)
	library(MasterProjectPackage)
	knit_hooks$set(document = function(x) {sub('\\usepackage[]{color}', '\\usepackage{xcolor}', x, fixed = TRUE)})
	opts_chunk$set(fig.path='figure/Report_figure-', cache.path='cache/latex-')
	options(width=70)
@

%-----------------------------------------------------------------------------------
%	TITLE INFORMATIONS
%-----------------------------------------------------------------------------------

 \begin{titlepage}
 \centering
 \includegraphics[width=.6\textwidth]{epfl}\par\vspace{1cm}
 \vspace{1cm}
 \textsc{\Large Master Thesis}\\[0.5cm] % Thesis type
 \vspace{1.5cm}
 \hrule
 \vspace{0.5cm}
 {\huge \bfseries Survival signature : an application on Carcinogenesis model  \par}
 \vspace{0.5cm}
 \hrule
 \vspace{1.5cm}
 {\huge\bfseries \par}
 \vspace{2cm}
 {\Large\itshape Rapha\"el Jauslin\par}
 \vfill
 supervised by\par
 Professor Stephan \textsc{Morgenthaler}
 \vfill
 {\large \today\par}
 \end{titlepage}

%-----------------------------------------------------------------------------------
%	TABLE OF CONTENTS
%-----------------------------------------------------------------------------------

\tableofcontents
\newpage
\listoffigures
\listoftables
\newpage
%-----------------------------------------------------------------------------------
%	INTRODUCTION
%-----------------------------------------------------------------------------------
 \section{Introduction}
\newpage


%-----------------------------------------------------------------------------------
%	MATHEMATICAL BACKGROUND
%-----------------------------------------------------------------------------------

\section{Mathematical background}

This section present the systems signature and its principal utilization. We introduce the concept of coherent system and the limitations of using the systems signature. It is a relatively new notion on the reliability theory domain and it is principally developed on the framework of engineering and economics \cite{Samaniego2007}. The notion of system must be understand as an abstract concept and could be represented many different object. In engineering this could be represent a machine with its components such as an airplane or a hi-fi stereo system \cite{barlow1975statistical}. Recently some works have been done about the utilization of signature to the concept of supply chain \cite{7093771}.\\

A system is a set of $n$ components that could take value in $\{0,1\}$. The value 0 or 1 represent respectively the fact that components has failed or currently working. We define the \textit{state vector} $\mathbf{x} = (x_1,...,x_n)\in\{0,1\}^n$ as the overall state components system. In order to know the relation between the components and the state system, we define the \textit{structure function} $\varphi : \{0,1\}^n \to \{0,1\}$ that map the state vector to the value 0 if the system has failed or 1 if the system is currently working \cite{Samaniego2007}. \\


\begin{figure}[!h]
	\centering
	\subfloat[Parallel system]{\label{fig:para5}\includegraphics[width=0.5\textwidth]{para5.png}}
	\subfloat[Series system]{\label{fig:serie5}\includegraphics[width=0.5\textwidth]{Serie5.png}}
	\hspace{5pt}
	\caption{Parallel and series system with five components. The system is functioning if there is path from the initial component $s$ to the last component $t$.}
	\label{fig:para5_serie5}
\end{figure}

In order to illustrate the concept and fix the idea we could look at some examples. Figure \ref{fig:para5} and \ref{fig:serie5} show the two easiest systems with five components, respectively the parallel system and the series systems. We call the $n$-components parallel system the one which have all of its components placed by parallel and the $n$-components series system the one with all its components on one line. The structure functions is represented by

\begin{equation}
 \varphi_{P}(\mathbf{x}) = 1- \prod_{i= 1}^{n} (1-x_i) = max(x_1,...,x_n)
 \label{eq:max}
\end{equation} 

and
\begin{equation}
\varphi_{S}(\mathbf{x}) = \prod_{i= 1}^{n} x_i = min(x_1,...,x_n).
\label{eq:min}
\end{equation}

These two systems are specified case of a more general class of system called a \textit{$k$-out-of-$n$}\cite{Samaniego2007,barlow1975statistical}. Meaning that $k$ components fail then we are sure that the system has failed. The structure function is,
$$\varphi(\mathbf{x}) = 0~~  \text{if}~~ \sum_{i = 1}^n x_i \leq n-k \text{ and } \varphi(\mathbf{x}) = 1 ~~ \text{if}~~ \sum_{i = 1}^n x_i \geq n-k +1.$$

Hence a parallel system is a $n$-out-of-$n$ system and the series system is a 1-out-of-$n$ system. In general the structure function is not find easily and even if the graph of the system is given, it is an hard task to manually find the structure function.\\ 


%-----------------------------------------------------------------------------------
%	COHERENT SYSTEM
%-----------------------------------------------------------------------------------


\subsection{Coherent system}
As a system could be a complex object with a great number of components, we need to introduce the concept of \textit{irrelevant} components. A component is said to be irrelevant to the system if it has no impact on the structure function. Formally, we could write this by,
$$ \varphi(x_1,...,x_{i-1},0,x_{i+1},...,x_n) = \varphi(x_1,...,x_{i-1},1,x_{i+1},...,x_n)$$
for all $(x_1,...,x_{i-1},x_{i+1},...,x_n) \in \{0,1\}^{n-1}$. So if a system contain a irrelevant component then the component could be remove and we would obtain a simpler system.\\

Another concept that we need to define a coherent system is the monotonicity of the structure function. The system (or equivalently the structure function) is said to be \textit{monotone} if $\varphi(\mathbf{x}) \leq \varphi(\mathbf{y})$ whenever $\mathbf{x}\leq\mathbf{y}$ \cite{Samaniego2007,barlow1975statistical}. On the system this means that it is not possible to change the state of system by replacing a working component by a failed component or just simply that it is not possible to improve the system by fixing a failed components. Then we have the following definition for a coherent system. 

\begin{defn}
	A system is said to be coherent if it contains no irrelevant components and the structure function is monotone.  
\end{defn} 

A direct conclusion from the definition of a coherent system is that we always have $\varphi(\mathbf{0}) = 0$ and $\varphi(\mathbf{1}) = 1$. Moreover it can be shown easily \cite{barlow1975statistical} that we have for an arbitrary coherent system with $n$ components that no system can perform better than the parallel and worse than the series system mathematically, the structure function is bounded as follow :
$$\prod_{i = 1}^n x_i \leq \varphi(\mathbf{x}) \leq 1- \prod_{i=1}^{n} (1-x_i).$$ 

In order to characterize the different coherent system of a certain order, we need to define the notion of \textit{path set} and \textit{cut set}. These two notion are closely related and are very useful to specified the coherent system \cite{Samaniego2007,barlow1975statistical}.\\

A \textit{path set} $P$ is a set of components of the system such that if they are all working then the system works. A path set is said to be minimal if it does not contain another path set. \\

A \textit{cut set} $C$ is a set of components of the system such that if they are all failed then the system fails. A cut set is said to be minimal if it does not contains another cut set.\\

Table \ref{tab:order3} and Figure \ref{fig:system_order_3} shows the minimal path and cut set with the duality and the graphic representation. Figure \ref{fig:System4} shows the 2-out-of-3 system. The number of coherent system is equal to 20 for the order 4 and grows exponentially in $n$ \cite{Samaniego2007}.\\
  
\begin{table}
	\centering
	\begin{tabular}{|c|c|c|c|}
		\hline
		System & Minimal path set & Minimal cut set & Dual system \\
		\hline
		1 & \{1\}, \{2\}, \{3\} & \{1,2,3\}& 5\\
		2 & \{1,2\},\{1,3\} & \{1\},\{2,3\}&  3\\
		3 & \{1\},\{2,3\} & \{1,2\}, \{1,3\}& 2\\
		4 & \{1,2\},\{2,3\},\{1,3\} & \{1,2\},\{2,3\},\{1,3\}& 4\\
		5 & \{1,2,3\} & \{1\}, \{2\}, \{3\} & 1 \\
	\hline
\end{tabular}
\caption{ Coherent system of order 3 with the specification of the minimal path and cut set.}
\label{tab:order3}
\end{table}

\begin{figure}[!h]
	\centering
	\subfloat[Parallel system]{\label{fig:System1}\includegraphics[width=0.33\textwidth]{System1.png}}~~~~
	\subfloat[Series system]{\label{fig:System5}\includegraphics[width=0.33\textwidth]{System5.png}}\\
	\subfloat[System 2]{\label{fig:System2}\includegraphics[width=0.33\textwidth]{System2.png}}
	\subfloat[System 3]{\label{fig:System3}\includegraphics[width=0.33\textwidth]{System3.png}}
	\subfloat[System 4]{\label{fig:System4}\includegraphics[width=0.33\textwidth]{System4.png}}
	\hspace{5pt}
	\caption{Coherent systems of order 3. The system is functioning if there is path from the initial component $s$ to the last component $t$.}
	\label{fig:system_order_3}
\end{figure}

These notions of path and cut set are crucial because one may represent every system with it. One can define a binary function called the \textit{minimal path series structure}\cite{barlow1975statistical} with arguments $x_i, i \in P_j$,
\begin{equation}
\rho_j(\mathbf{x}) = \prod_{i \in P_j} x_i
\label{eq:rho}
\end{equation}

this function is the series arrangement of the $j$th minimal path set. Moreover, because the system function if and only if at least one of the minimal path set is functioning then we could rewrite the structure function as,

\begin{equation}
 \varphi(\mathbf{x}) = 1- \prod_{j = 1}^p(1-\rho_j(\mathbf{x}))
 \label{eq:pathserie}
\end{equation}

where $p$ is the total number of minimal path set. A similar derivation could be done with the cut set and we obtain the \textit{minimal cut parallel structure} \cite{barlow1975statistical} with $x_i, i \in C_j$,
\begin{equation}
\kappa_j(\mathbf{x}) = 1- \prod_{i \in C_j} (1-x_i)
\label{eq:kappa}
\end{equation}
and 
\begin{equation}
 \varphi(\mathbf{x}) = \prod_{j = 1}^c \kappa_j(\mathbf{x}).
 \label{eq:cutparallel}
\end{equation}
where $c$ is the total number of minimal cut set in the system. By the previous definition of \eqref{eq:max} and \eqref{eq:min} we could then rewrite the function \eqref{eq:pathserie} by 
\begin{equation}
\varphi(\mathbf{x}) = \max_{1\leq j \leq p}\rho_j(\mathbf{x}) = \max_{1 \leq j\leq p} \min_{i \in P_j} x_i
\label{eq:maxmin}
\end{equation}
and \eqref{eq:cutparallel} as 
\begin{equation}
\varphi(\mathbf{x}) = \min_{1\leq j \leq c}\kappa_j(\mathbf{x}) = \min_{1 \leq j\leq c} \max_{i \in C_j} x_i
\label{eq:minmax}
\end{equation}




%-----------------------------------------------------------------------------------
%	SYSTEM SIGNATURE
%-----------------------------------------------------------------------------------


\subsection{System signature}

 We define $T_{sys} > 0$ the random \textit{failure time of the system} and $T_{j:n}$ the $j$-th random \textit{failure time of the} $n$ \textit{components}.


\begin{defn}
	Assuming that the failure time components of a coherent system are independent and identically distributed, the \textit{system signature} is define as the vector $q \in [0,1]^n$ with the $j$-th component equal to
	$$ q_j = \pr(T_{sys} = T_{j:n}). $$ 
Hence $q_j$ represents the probability that the  $j$-th component failure causes the whole system failure. 	
\end{defn}

We will assume moreover that $\sum_{i = 1}^n q_j = 1$ which means that the system function if all of its components function. The assumption about the independent and identically distributed failure time is made for convenience but could be relaxed to exchangeability \cite{Coolen2012}. 
\begin{defn}
	An infinite sequence of random variable $X_1,X_2,...$ is said to be exchangeable if for all $n \in \mathbb{N}$, $\{1,...n\}$  and a permutation of it  $\{\sigma(1),...,\sigma(n)\}$, $(X_1,...,X_n)$ and $(X_{\sigma(1)},...,X_{\sigma(n)})$ have the same distribution.
\end{defn}

We can proof that independent and identically distributed is closely related \cite{durrett2010probability}. The theorem is called the De Finetti's theorem in honor of Bruno De Finetti. See \cite{durrett2010probability} to have more informations and some transversal citations that is useful to understand all the theory about the concept of exchancheability for instance the extensions to the finite case. For the rest of this report we will in general supposed the independant and identically distributed assumptions but we should keep in mind that this could be reduced to the weaker assumptions of exchangeable random variables. \\

 In practice the system signature is used to compare the reliability of some systems designs \cite{Samaniego2007}, hence one of the main reasons of the assumptions could be seen as identifiability. For example, if we do not supposed exchangeability we obtain clearly that two systems will be comparable by the characteristics of its component and by the systems designs. This could induced confusion about the reliability, hence these assumptions are made to have that a comparison of systems are only consequences of system designs. We could then define the survival function of the system by the following theorem :\\

\begin{thrm}
	Let $T_1,...,T_n$ be i.i.d component lifetimes of an $n$-component coherent system with signature $\mathbf{q}$, and let $T_{sys}$ be the system's lifetime. Then,
	\begin{equation}
		 \pr(T_{sys} > t) = \sum_{i = 1}^n q_i\sum_{j=0}^{i-1} \binom{n}{j}F(t)^j (1-F(t))^{n-j}.
	\end{equation}
\label{thrm:sig}
\end{thrm}
\begin{proof}
	First we note that the system can fail only when occur a component failure. Formally, we have that $T_{sys} \in \{T_{1:n},...,T_{n:n}\}$ with probability 1. Hence, by the Total Probability and Bayes Theorem, we obtain :
	$$\begin{array}{lll}
	\pr(T_{sys} > t) &=&\D \sum_{i=1}^n \pr(T_{sys} > t, T_{i:n} = t)\\
	&=&\D \sum_{i=1}^n \pr(T_{sys} > t| T = T_{i:n})\pr(T_{sys} = T_{i:n})\\
	&=&\D \sum_{i=1}^n q_i\pr(T_{i:n} > t)\\	
	&=&\D \sum_{i=1}^n q_i \sum_{j = 0}^{i-1} \binom{n}{j} F(t)^j(1-F(t))^{n-j}\\
	\end{array}$$
\end{proof}

A particularity of the Theorem \ref{thrm:sig} is that we could represent the reliability of the system by
\begin{equation}
\pr(T_{sys} > t) = \sum_{i = 1}^n q_i \pr(T_{i:n} >t),
\end{equation}
and from the well-known representation of the expected value for positive random variable $X$
\begin{equation}
 \mathbb{E}[X] = \int_0^\infty \pr(X > x) dx
\end{equation}
We obtain that the expected failure time of the system is represented by:
\begin{equation}
\E[T] = \sum_{i = 1}^n q_i \E[T_{i:n}].
\end{equation}


%-----------------------------------------------------------------------------------
%	LIMITATIONS OF THE SYSTEMS SIGNATURE
%-----------------------------------------------------------------------------------

\subsection{Limitations of the system signature}

The system signature has shown its utility in reliability theory and is useful for systems that have a single type of components \cite{Coolen2012}. We will show in this section that the generalization of the system signature for multiple types components is extremely complicated \cite{Coolen2016}.\\

We will denote $K\geq 2$ to be the number of type of components, $m_k$ for the number of type $k$ with $k\in \{1,...,K\}$ and $\sum_{k=1}^K m_k = m$ where $m$ will be the total number of components. The state vector of the system is represented by $\mathbf{x} = (\mathbf{x}^1,...,\mathbf{x}^K)$ with $\mathbf{x}^k = (x_1^k,...,x_{m_k}^k)$ the state vector of the components of type $k$. Hence, if we suppose for ease of notation that ties between failure times have a zero probability of occurring, the ordered failure times of the $m_k$ components of type $k$ is noted as $T_{j_k:m_k}^k$. In order to satisfies the exchancheable (or i.i.d) assumption made for the single component case, we will suppose exchangeable (or i.i.d) in each type of components $k$, but we will need to take into account how many of the components of each other type have failed.\\

The generalized total signature is then defined by :

\begin{equation}
q = (q_1(1),...,q_1(m_1),...,q_K(1),...,q_K(m_K))
\label{eq:genezalizedtotalsig}
\end{equation}  
where $k \in \{1,...,K\}$ and
\begin{equation}
q_k(j_k) = \pr(T_{sys} = T_{j_k:m_k})  ~~\text{ with }~ j_k \in {1,...,m_k}.
\label{eq:gensig}
\end{equation}
The survival could then be derived exactly as it is done in the Theorem \ref{thrm:sig} and we obtain :
\begin{equation}
\pr(T_{sys} > t) = \sum_{k = 1}^K\sum_{j_k = 1}^{m_k} q_k(j_k)\pr(T_{j_k:m_k}^k >t).
\label{eq:gensyssig}
\end{equation}

The derivation of \eqref{eq:gensyssig} is an hard task even infeasible when the number of components $K$ become to be large \cite{Coolen2012}. Indeed, it involves order statistics from different probability distributions and moreover the separation of the system structure and components distributions is no longer achieve in this formula.\\

We will now define a similar function with as far as good properties than the system signature and more. For more informations and notions about the system signature, structure function or reliability theory we refer to \cite{Samaniego2007},\cite{barlow1975statistical}, \cite{Coolen2012}.





%-----------------------------------------------------------------------------------
%	SURVIVAL SIGNATURE
%-----------------------------------------------------------------------------------
\newpage
\section{Survival signature}
One of the most limitations of the system signature is the specification of reliability in systems with multiple type of components. In order to facilitate the incorporation of multiple type of components and the modeling differentiation between the system structure and the components distribution, Coolen and Coolen-Maturi define the survival signature \cite{Coolen2012} .

\begin{defn}
Let $K\geq1$ be the type of components and $m_k$ be the number of components of type $k \in \{1,...,k\}$ included in the system. Suppose that for a fixed type of components the failure times are independent and identically distributed. Let $\varphi (\mathbf{x})$ be the structure function of the system and $m = \sum_{k = 1}^K m_k$ with $\mathbf{x} = (\mathbf{x}^1,...,\mathbf{x}^K) \in \{0,1\}^m$ the state vector of the all system and $\mathbf{x}^k= (x_1^k,...,x_{m_k}^k)$ the state vector of the components of type $k$. We define the system survival signature as

\begin{equation}
\phi(l_1,...,l_k) =  \left[ \prod_{k = 1}^{K}\binom{m_k}{l_k}^{-1}\right] \sum_{\mathbf{x}\in S_{l_1,...,l_K}} \varphi(\mathbf{x}),
\label{eq:survsig}
\end{equation}

where $S_{l_1,...,l_K}$ is the set of all state vector for which $\sum_{i = 1}^{m_k} x_i^k = l_k$. Hence the survival system signature or just the survival signature is the probability that the system functions given that precisely $l_k$ components of the type $k$ is working.
 
\end{defn} 

In order to define the reliability of the system, let $C_t^k \in \{0,1,...,m_k\}$ represent the number of components of type $k$ functioning at time $t>0$. Suppose that the components of type $k$ have known cumulative distribution function $F_k(t)$. Let $\mathbf{l} = (l_1,...,l_K), l_k \in \{0,1...,m_k\}$ be the number of functioning components of the different type in the system, we then define the probability that the system have the value of the vector $\mathbf{l}  = (l_1,...,l_K)$ by 
\begin{equation}
\pr\left(\bigcap_{k=1}^K \{C_t^k = l_k\}\right) = \prod_{k= 1}^K \pr(C_t^k = l_k) = \prod_{k=1}^K \left(\binom{m_k}{l_k} F_k(t)^{m_k - l_k} (1-F_k(t))^{l_k}\right)
\label{eq:probintersurvsig}
\end{equation}

The last equation comes from the fact that $C_t^k$ can be represented by a binomial random variable with parameter equal to $m_k$ and probability $F_k(t)$. Indeed,
\begin{equation}
C_t^k = \sum_{i = 1}^{m_k} \mathbb{I}(X_i^k|t = 1)~ \text{ where } X_i^k|t = \left\{\begin{array}{lll} 1 &\text{ if }& i\text{th component of type k works at time }t \\
0 &\text{ if }& i\text{th component of type k does not works at time }t\\  \end{array}\right.
\label{eq:Ctk}
\end{equation}
and $\mathbb{I}(.)$ is the indicator function that take value 1 or 0 depending on whether the statement $(.)$ is true or false. The indicator variable  $\mathbb{I}(X_i^k|t = 1)$ takes value 1 with probability $F_k(t)$.\\

The survival function of the system could then be define as

\begin{equation}
\pr(T_{sys} > t) = \sum_{l_1 = 0}^{m_1} \cdots \sum_{l_K = 0}^{m_K} \phi(l_1,...,l_K)\pr\left(\bigcap_{k = 1}^K \{C_t^k = l_k\}\right)
\label{eq:relisurvsig1} 
\end{equation}
by the formula \eqref{eq:probintersurvsig} we could rewrite this as
\begin{equation}
\pr(T_{sys} > t) = \sum_{l_1 = 0}^{m_1} \cdots \sum_{l_K = 0}^{m_K} \left[\phi(l_1,...,l_K) \prod_{k=1}^K \left(\binom{m_k}{l_k} F_k(t)^{m_k - l_k} (1-F_k(t))^{l_k}\right)\right]
\label{eq:relisurvsig2} 
\end{equation}

The greatest advantage of using \eqref{eq:relisurvsig2} instead of \eqref{eq:gensyssig} is that the system structure informations is separated of the components informations. Moreover it not longer need to calculate probability of ordered statistics. This clearly does not mean that it is easily find but it is easier. Another interesting thing that come up with the equation \eqref{eq:probintersurvsig}, is that we do the assumption of independence among the failure times of components of different types but this assumptions could be relaxed.\\

Figure \ref{fig:system_example} and Table \ref{tab:system_example} show an example of a simple coherent system of order 4, this system is interesting by the fact that it has an asymmetrical survival signature. Indeed it is easy to see that the survival signature for the values $(0,2)$ is not equal to the value $(2,0)$. This effect could be a real interpretation such has if the two components of type one are inactivated then it is absolutely sure that the system is not working although if the two components of type 1 are working but the components of type 2 are inactivated then the system still working. This kind of situation are really supposed to be possible in a biological context and the concept of signature allow an easy incorporation of it. 
\begin{figure}[!h]
	\centering
\includegraphics[width=0.9\textwidth]{SystemExample}
\caption{Coherent system of order 4 with minimal path set $\{1,2\},\{1,3\},\{2,3,4\}$. }
\label{fig:system_example}
\end{figure}

\begin{table}
	\centering
	\begin{tabular}{c c|c}
		 $l_1$ & $l_2$ & $\phi(l_1,l_2)$ \\
		\hline
		0 & 0 & 0 \\
		0 & 1 & 0\\
		0 & 2 &  0\\
		1 & 0& 1/2\\
		1 & 1 & 3/4\\
		1 & 2 & 1\\
		2 & 0 & 1\\
		2 & 1 & 1\\
		2 & 2 & 1\\
	\end{tabular}
	\caption{Survival signature of the system describe in Figure \ref{fig:system_example}.}
	\label{tab:system_example}
\end{table}


%-----------------------------------------------------------------------------------
%	SURVIVAL SIGNATURE AS COMBINATION OF SUBSYSTEMS
%-----------------------------------------------------------------------------------

\subsection{Survival signature as combinations of subsystems}
In this section we will discuss another way to find the survival signature that could possibly reduce the time consuming estimation if the number of components is huge. First of all let suppose that we have two subsystems in series configuration with the same single type of components. Let $m^r$ be the number of component of the subsystem such that $m = m^1 + m^2$. Then the survival signature is find by the following combination :

\begin{equation}
\phi(l) = \binom{m}{l}^{-1} \left[ \sum_{l^1 = 0}^l \binom{m^1}{l^1} \binom{m^2}{l-l^1}\phi^1(l^1)\phi^2(l-l^1) \right].
\label{eq:combi_single_serie}
\end{equation}   

To find this equation it suffice to use the Law of total probability and the hypergeometric distribution. Indeed $\phi(l)$ represents the probability that the system function when $l$ components functions, $\phi^1(l^1)$ represents the probability that the subsystem function when $l^1$ components functions. For a ease of notation let $\phi^1(l^1) = 0$ (respectively $\phi(l-l^1) = 0$) for the value where $l^1 > m^1$ (respectively $l- l^1 > m^2$) \cite{Abdu2014}. Then the probability for the event that $l^1$ of the $l$ components functioning components are in the subsystem 1 and that $l-l^1$ are in the subsystem 2 is equal by the hypergeometric probability to :
$$ \frac{\binom{m^1}{l^1}\binom{m^2}{l-l^1}}{\binom{m}{l}}. $$
Finally, to get the formula \eqref{eq:combi_single_serie} knowing the different combinations between the components we only need to see that the subsystem are in series position. The two subsystem needs to functions if we would like to have the whole system functioning, hence the two probability $\phi^1(l^1)$ and $\phi^2(l-l^1)$ multiplies to get the right structure. It is enough by the Law of total probability to sum over the possible combinations. Similarly, if the subsystem are in parallel position, we get the following equation :

\begin{equation}
\phi(l) = \binom{m}{l}^{-1} \left[ \sum_{l^1 = 0}^l \binom{m^1}{l^1} \binom{m^2}{l-l^1}\left\{1-(1-\phi^1(l^1))(1-\phi^2(l-l^1))\right\} \right].
\label{eq:combi_single_parallel}
\end{equation} 

The only thing that differs form the equation \eqref{eq:combi_single_serie}, it is that for each possible combination the either the subsystem 1 or the subsystem 2 need to functioning. This probability is find by calculate the complementary probability of the event that the two systems does not function which is $(1-\phi^1(l^1))(1-\phi^2(l-l^1))$.\\

We can generalize when there is more than single type of components but we still have only two subsystem. For ease of notation we let the quantity $\phi(l_1^r,...,l_K^r) = 0$ if $l_k^r > m_k^r$ for $r = 1,2$. Then if the subsystem are in series configuration, we get the survival signature for the whole system equal to :

\begin{eqnarray}
\phi(l_1,...,l_K) &=& \sum_{l_1^1 = 0}^{l_1} \cdots \sum_{l_K^1 = 0}^{l_K} \Bigg[\phi^1(l_1^1,...,l_K^1)\phi^2(l_1-l_1^1,...,l_K - l_K^1)\times  \nonumber \\
&& \prod_{k = 1}^K \binom{m_k^1}{l_k^1} \binom{m_k^2}{l_k-l_k^1} \binom{m_k}{l_k}^{-1}\Bigg].
\label{eq:combi_multi_serie}
\end{eqnarray} 

The derivation comes from exactly the same combinatorial aspect as for the equation \eqref{eq:combi_single_serie}. For the parallel configuration we get :

\begin{eqnarray}
\phi(l_1,...,l_K) &=& \sum_{l_1^1 = 0}^{l_1} \cdots \sum_{l_K^1 = 0}^{l_K} \Bigg[\big\{1-(1-\phi^1(l_1^1,...,l_K^1))(1-\phi^2(l_1-l_1^1,...,l_K - l_K^1))\big\}\times  \nonumber \\
&& \prod_{k = 1}^K \binom{m_k^1}{l_k^1} \binom{m_k^2}{l_k-l_k^1} \binom{m_k}{l_k}^{-1}\Bigg].
\label{eq:combi_multi_parallel}
\end{eqnarray}

The generalization of system with more than two subsystems can be done by composition of the last equations. It is only possible if the system are either in series or in parallel. Altough this can be used to derive the system's survival signature for many complicated systems \cite{Abdu2014}.

%-----------------------------------------------------------------------------------
%	NONPARAMETRIC P. I.
%-----------------------------------------------------------------------------------

\subsection{Nonparametric predictive inference}
In order to understand the concept of nonparametric predictive inference  (NPI)\footnote{\url{http://www.npi-statistics.com/}} \cite{Cool2011}, we need to define the Hill's assumption.

\begin{defn}(Hill's assumption)
Suppose that $X_1,...,X_n,X_{n+1}$ are continuous and exchangeable random quantities. Let the ordered observed values of $X_1,...,X_n$ by denoted by $ x_{(1)} < ... < x_{(n)} < \infty$ and let $x_{(0)} = - \infty$ and $x_{(n+1)} = \infty$ for ease of notation. For a future observation $X_{n+1}$ based on $n$ observations, the Hill's assumption $A_{(n)}$ is 
\begin{equation}
\pr(X_{n+1}\in (x_{(j-1)},x_{(j)})) = \frac{1}{n+1} \text{ for } j = 1,2,...,n+1
\end{equation}
\end{defn}

The Hill's assumptions assert that conditionally on the variable $X_1,...,X_n$, $X_{n+1}$ equally likely is falling in any open intervals between successive order statistics of the given sample \cite{Bruce}. The statistical method is inference based on $A_{(n)}$ which is predictive and nonparametric and should be used if some bounds are wanted for probability events of interest \cite{Cool2011}.\\

First of all we derive the notion of NPI for Bernoulli quantities a good explanation on it can be find in the PhD Thesis of  \cite{Aboalkhair2012} and \cite{Abdu2014}, the next paragraphs will be based on this work. The purpose on Bernoulli quantities will be used later for the definition of the NPI for the reliability of systems based on survival signature \cite{Coolen2014}.

\begin{defn} (NPI for Bernoulli quantities)
	Suppose that there is a sequence of $n+m$ exchangeable Bernoulli trials, each with success and failure  as possible outcomes, and data consisting of $s$ successes in $n$ trials. Let $Y_1^n$ denote the random number of successes in trials 1 to $n$ and let $Y_{n+1}^{n+m}$ denote the random number of successes in trials $n+1$ to $n+m$. Let $R_t \in \{r_1,...r_t\}$, with $1\leq t \leq m+1$ and $0\leq r_1 < r_2 < ... <r_t \leq m$ and, for ease of notation, define $\binom{s + r_0}{s} = 0$. Then the NPI upper probability for the event $Y_{n+1}^{n+m} \in R_t$, given the data $Y_1^n = s$, for $s\in \{0,...,n\}$, is 
	\begin{equation}
	\overline{\pr}(Y_{n+1}^{n+m}\in R_t | Y_1^n = s) = \binom{n+m}{n}^{-1} \times\sum_{j = 1}^t\left[  \binom{s+r_j}{s} - \binom{s+r_{j-1}}{s} \right] \binom{n-s+m-r_j}{n-s}
	\label{eq:upperNPIBern}
	\end{equation}
	The corresponding NPI lower probability can be derived via the conjugacy property 
	 \begin{equation}
	 \underline{\pr}(Y_{n+1}^{n+m}\in R_t | Y_1^n = s) = 1- 	\overline{\pr}(Y_{n+1}^{n+m}\in R_t^c | Y_1^n = s)
	 \label{eq:lowerNPIBern} 
	\end{equation}
	where $R_t^c = \{0,1,...,m\}$\textbackslash $R_t$ .
\label{def:NPIbern}
\end{defn}

To find more information on how the equation \eqref{eq:upperNPIBern} is derived we refer to \cite{COOLEN1998349}. We can now derive the NPI lower and upper probabilities survival function for the failure time $T_{sys}$. NPI is used for learning about components of a specific type in the system, from data consisting of failure times for components that are exchangeable with these \cite{Coolen2014}. We will suppose then that we have a data set such that $n_k$ represent the number of components of type $k$ for which test failure data are available, $s_k(t)$ denote the number of these components which still functioning at time $t$. Then the NPI lower bound of the survival function is define by :

\begin{equation}
\underline{\pr}(T_{sys} > t) = \sum_{l_1 = 0}^{m_1} \cdots \sum_{l_K = 0}^{m_K} \phi(l_1,...,l_K) \prod_{k= 1}^K \overline{\mathbb{D}}(C_k(t) = l_k)
\label{eq:lowerNPIsurvsig}
\end{equation}
where 
\begin{equation}
\overline{\mathbb{D}}(C_k(t) = l_k) = \overline{\pr}(C_k(t) \leq l_k) - \overline{\pr}(C_k(t) \leq l_k-1) 
\end{equation}
and $\overline{\pr}$ denote the NPI upper probability for Bernoulli data. Similarly we have that the NPI upper bound for the survival function is define by :

\begin{equation}
\overline{\pr}(T_{sys} > t) = \sum_{l_1 = 0}^{m_1} \cdots \sum_{l_K = 0}^{m_K} \phi(l_1,...,l_K) \prod_{k= 1}^K \underline{\mathbb{D}}(C_k(t) = l_k)
\label{eq:upperNPIsurvsig}
\end{equation}
where 
\begin{equation}
\underline{\mathbb{D}}(C_k(t) = l_k) = \underline{\pr}(C_k(t) \leq l_k) - \underline{\pr}(C_k(t) \leq l_k-1) 
\end{equation}
and $\underline{\pr}$ denote the NPI lower probability for Bernoulli data.

\begin{prop}
	\begin{equation}
	\overline{\pr}(C_k(t) \leq l_k) - \overline{\pr}(C_k(t) \leq l_k-1) = \binom{n_k + m_k}{n_k}^{-1}\binom{s_k(t) -1 +l_k}{s_k(t) -1} \binom{n_k -s_k(t) + m_k - l_k}{n_k - s_k(t)}
	\end{equation}
\end{prop}
\begin{proof}
From the definition \ref{def:NPIbern}, let $R_t = {r_1,...,r_t}$ such that $r_t = l_k$ hence clearly $r_{t-1} = l_k -1 $. By the following direct calculation, we obtain the desired result. To simplify notation we will denote $s_k(t) = s$, $n_k = n$ and $m_k = m$.

$$\begin{array}{lll}
	\overline{\pr}(C_k(t) \leq l_k) - \overline{\pr}(C_k(t) \leq l_k-1) &=& \binom{n + m}{n}^{-1} \sum_{j = 1}^t \left[ \binom{s + r_j}{s} +  \binom{s + r_{j-1}}{s} \right] \binom{n-s + m -r_j}{n-s}\\
	&& - \sum_{j = 1}^{t-1} \left[ \binom{s + r_j}{s} +  \binom{s + r_{j-1}}{s} \right] \binom{n-s + m -r_j}{n-s}\\
	& =& \binom{n + m}{n}^{-1} \left[ \binom{s + r_t}{s}  - \binom{s + r_{t-1}}{s} \right] \binom{n-s + m -r_t}{n-s}\\
	& =& \binom{n + m}{n}^{-1} \left[ \binom{s + l_k}{s}  - \binom{s + l_k - 1}{s} \right] \binom{n-s + m -l_k}{n-s}	\\
\end{array}$$
	
It is easy to verify that 
$$ \binom{s + l_k}{s}  - \binom{s + l_k - 1}{s}  = \binom{s-1 +l_k}{s-1}.$$
\end{proof}

\begin{table}
	\centering
	\begin{tabular}{c c|c | c}
		$l_1$ & $l_2$ & $\phi^B(l_1,l_2)$ & $\phi(l_1,l_2)$ \\
		\hline	
		0 & 0 & 0 &0 \\
		0 & 1 & 0 & 0\\
		0 & 2 &  [0,1]  &0\\
		1 & 0& [0,1] & 1/2\\
		1 & 1 & [0,1]& 3/4\\
		1 & 2 & [0,1]& 1\\
		2 & 0 & [0,1]& 1\\
		2 & 1 & 1& 1\\
		2 & 2 & 1& 1\\
	\end{tabular}
	\caption{Survival signature of the system describe in Figure \ref{fig:system_example} and bounds on the survival signature used for the Figure \ref{fig:SystemExample}}
	\label{tab:system_example_bound}
\end{table}

One of the main interesting application of the NPI lower and upper probabilities for the reliability of a system is the bounds on survival signatures. One could potentially imagine that we have a system with a huge number of components and then find the exact survival signature is a too complex problems but one could imagine that algorithm can bounded the survival signature with less computing time \cite{Abdu2014}. Let denote $\phi^B$ the survival function of a system with bounded value noted as intervals. Table \ref{tab:system_example_bound} is an example of value with bounded survival signature for the system specified in Figure \ref{fig:system_example}. We compute the NPI upper bound of the upper bound of the survival signature and the NPI lower bound of the lower bound of the survival signature. This gives us four function noted as $\overline{\pr}^U(T_{sys} >t)$, $\overline{\pr}^L(T_{sys} >t)$, $\underline{\pr}^U(T_{sys} >t)$ and  $\underline{\pr}^L(T_{sys} >t)$. In order to get the wider interval we look at the functions  $\underline{\pr}^L(T_{sys} >t)$ and  $\overline{\pr}^U(T_{sys} >t)$. Figure \ref{fig:SystemExample} shows the two functions with additionally the NPI lower and upper bound with the exact survival signature and the analytical reliability calculated from the system with components of type 1 with a Weibull distribution with shape parameter equals to 2.5 and scale parameter equal 50, the components of type 2 is as well Weibull distributed with shape parameter equals to 5 and scale parameter equal 100. The NPI functions are estimated with a set of 100 sample failure time from the two distributions.\\
\begin{figure}[!h]
\centering
<<SystemExample,echo=FALSE,fig.height=5,warning=FALSE,message=FALSE,results='hide',cache = TRUE>>=
## System information
g <- graph.formula(s-1-2-t,s-1-3-t,s-2-3-4-t)
g <- setCompTypes(g,list("T1" = c(1,2),"T2"=c(3,4)))
survsig <- computeSystemSurvivalSignature(g)

survsigLow <- survsig
survsigUp <- survsig
survsigLow$Probability <- c(0,0,0,
0,0,0,
0,1,1)
survsigUp$Probability <- c(0,0,1,
1,1,1,
1,1,1)

n <- 100
t <- c(rweibull(n,shape = 2.5,scale = 50),rweibull(n,shape = 5,scale = 100))
ord <- c(rep(1,n),rep(2,n))
timeline <- data.frame(t = t,ord = ord)
timeline <- timeline[order(t),]

Ncomp <- c(n,n)
OrderedFailureTime <- timeline$t
CompIndex <- timeline$ord


NPILow <- NPIRelib(survsigLow,OrderedFailureTime,CompIndex,Ncomp)
NPIexact <-  NPIRelib(survsig,OrderedFailureTime,CompIndex,Ncomp)
NPIUp <- NPIRelib(survsigUp,OrderedFailureTime,CompIndex,Ncomp)


F_t <- list(pweibull,pweibull)
paramF1 <- list(shape = 2.5,scale = 50)
paramF2 <- list(shape = 5,scale = 100)

param <- list(paramF1,
paramF2)

mesh <- seq(0,100,0.1)
r <- Relib(survsig,mesh,F_t,param)


dat <- data.frame(x = c(mesh),
y = c(r),
type = c(rep("Analytic",length(mesh))),
Label = c(rep("Analytic",length(mesh)))
)


datexact <- data.frame(x = c(NPIexact$t,NPIexact$t),
y = c(NPIexact$lower,NPIexact$upper),
type = c(rep("NPIExactlow",length(NPIexact$t)),rep("NPIExactup",length(NPIexact$t))),
Label =  c(rep("NPIExact",length(NPIexact$t)),rep("NPIExact",length(NPIexact$t))))

datNPI <- data.frame(x = c(NPIUp$t,NPILow$t),
y = c(NPIUp$upper,NPILow$lower),
type = c(rep("NPIUp",length(NPIUp$t)),rep("NPILow",length(NPILow$t))),
Label = c(rep("NPI",length(NPIUp$t)),rep("NPI",length(NPILow$t))))

cols <- c("NPI" = "lightskyblue3", "NPIExact" = "lightskyblue4","Analytic" = "ivory4")


ggplot() + 
geom_step(dat = datexact,aes(x = x, y =y, group = type,color = Label))  +
geom_step(dat = datNPI,aes(x = x,y = y,group = type,color = Label)) +
geom_line(dat = dat,aes(x = x,y = y,group = type,color = Label)) +
scale_colour_manual(values = cols) +
ggtitle("NPI and bounds on survival signature") + 
xlab("Failure times") + ylab("Reliability") +
theme_light() +  theme(plot.title = element_text(hjust = 0.5))

@
\caption{Reliability function of the system of order 4 illustrated in Figure \ref{fig:system_example} with the survival signature in Table \ref{tab:system_example_bound}. The failure time is sampled from Weibull distribution of parameter 2.5 and 50 for the component 1 and 5 and 100 for the components 2.}
\label{fig:SystemExample}
\end{figure}

One can observe that the NPI lower and upper bound does not contain the analytical solution. As explain previously, the imprecise probability in the NPI lower bound \eqref{eq:lowerNPIsurvsig} and the NPI upper bound \eqref{eq:upperNPIsurvsig} are estimated with Bernoulli random quantity. Then there is no assurance that the time realizations of the random variable gives us bound on the analytical solution. The NPI should be used when there is quasi no informations on the component distribution and with only partial information on the exact survival signature \cite{Abdu2014}. One things that one can illustrate is that the NPI lower and upper bound converge as the number of failure time increase. Figure \ref{fig:NPIExample} shows the NPI lower and upper bound for different set of sample failure time with two Weibull distribution with shape parameter equals to 2.5 for the components 1 and 5 for the components 2 and scale parameter respectively equals to 50 and 100. We see that the NPI converge to the analytical solution as the sampled time converge to the distribution function.

\begin{figure}[!h]
\centering
<<NPIExample,echo=FALSE,fig.height=5,warning=FALSE,message=FALSE,results='hide',cache = TRUE>>=
## System information

g <- graph.formula(s-1-2-t,s-1-3-t,s-2-3-4-t)
g <- setCompTypes(g,list("T1" = c(1,2),"T2"=c(3,4)))
survsig <- computeSystemSurvivalSignature(g)

F_t <- list(pweibull,pweibull)
paramF1 <- list(shape = 2.5,scale = 50)
paramF2 <- list(shape = 5,scale = 100)

param <- list(paramF1,
paramF2)

# NPI info
n <- c(8,20,50,125)
npi <- list()
for(i in 1:length(n)){
t <- c(rweibull(n[i],shape = 2.5,scale = 50),rweibull(n[i],shape = 5,scale = 100))
ord <- c(rep(1,n[i]),rep(2,n[i]))
timeline <- data.frame(t = t,ord = ord)
timeline <- timeline[order(t),]

Ncomp <- c(n[i],n[i])
OrderedFailureTime <- timeline$t
CompIndex <- timeline$ord
npi[[i]] <- NPIRelib(survsig,OrderedFailureTime,CompIndex,Ncomp)
}

mesh <- seq(0,100,0.1)
r <- Relib(survsig,mesh,F_t,param)

# data.frame
dat0 <- data.frame(Time = mesh,r = r)
#type = rep("Exact"),length(mesh))

dat1 <- data.frame(Time = npi[[1]]$t,up = npi[[1]]$upper,low= npi[[1]]$lower,type = rep("8",length(npi[[1]]$t)))
dat2 <- data.frame(Time = npi[[2]]$t,up = npi[[2]]$upper,low= npi[[2]]$lower,type = rep("20",length(npi[[2]]$t)))
dat3 <- data.frame(Time = npi[[3]]$t,up = npi[[3]]$upper,low= npi[[3]]$lower,type = rep("50",length(npi[[3]]$t)))
dat4 <- data.frame(Time = npi[[4]]$t,up = npi[[4]]$upper,low= npi[[4]]$lower,type = rep("125",length(npi[[4]]$t)))

datall <- c()
datall <- rbind(dat1,dat2,dat3,dat4)

cols <- c("NPI" = "lightskyblue3", "NPIExact" = "lightskyblue4","Analytic" = "ivory4")

# plot
ggplot() +
geom_step(data = datall,aes(x = Time,y = low),colour = "lightskyblue4") +
geom_step(data = datall,aes(x = Time,y = up),colour = "lightskyblue4")+
geom_line(data = dat0,aes(x = Time,y = r),colour = "ivory4" )+
facet_wrap(~type,ncol = 2) +
ggtitle("NPI convergence illustration") + 
xlab("Failure times") + ylab("Reliability") +
theme_light() +  theme(plot.title = element_text(hjust = 0.5))

@
\caption{Convergence illustration of the NPI lower and upper bound. reliability function of the system of order 4 illustrated in Figure \ref{fig:system_example} with the survival signature in Table \ref{tab:system_example_bound}. The failure time is sampled from two Weibull distribution of parameter 2.5 and 50 for the component 1 and 5 and 100 for the components 2.}
\label{fig:NPIExample}
\end{figure}


%-----------------------------------------------------------------------------------
%	BAYESIAN P. I.
%-----------------------------------------------------------------------------------

\subsection{Components partial information}
The NPI lower and upper bound gives us good inference methods when there is some partial informations on the survival signature. It is clearly nonparametric inference methods and it is useful when there is a huge number of components as the estimation of the survival signature could be really time consuming \cite{Coolen2014}.\\ 

Here we will discuss another way of estimate bounds on the reliability function but in a more parametric point of view (see \cite{FENG2016116}). One can imagine that we have some informations on the distributions of the components but we only have few informations on the parameters. Mathematically, let $F_k(t|\theta)$ be the distribution of the $k$ components depending on the parameters $\theta \in [\theta^l,\theta^u]$. Suppose that we only know the bound of the parameters, this leads to two distribution of the components $\overline{F_k}(t) =F_k(t|\theta^u)$ and $\underline{F_k}(t) = F_k(t|\theta^l)$. To fix the idea let suppose that all $m$ components of a system is distributed exponentially then $F_k(t|\lambda_k) = 1-e^{\lambda_k t}$ for all $k = 1,...,m$ and $\lambda_k \in [\lambda_k^l,\lambda_k^u]$. Then the lower and upper bound of the reliability function of the system is given by the two functions :
\begin{equation}
\underline{\pr}(T_{sys} > t) = \sum_{l_1 = 0}^{m_1}\cdots \sum_{l_K = 0}^{m_K} \phi(l_1,...,l_K) \prod_{k = 1}^K \binom{m_k}{l_k} (1-e^{-\lambda_k^l})(e^{-\lambda_k^l})^{l_k}
\end{equation}
and 
\begin{equation}
\overline{\pr}(T_{sys} > t) = \sum_{l_1 = 0}^{m_1}\cdots \sum_{l_K = 0}^{m_K} \phi(l_1,...,l_K) \prod_{k = 1}^K \binom{m_k}{l_k} (1-e^{-\lambda_k^u})(e^{-\lambda_k^u})^{l_k}
\end{equation}

If the component distribution is not exponential but depends on more parameters like Weibull or Gamma. Then one need to care about on which parameters we have imprecise informations. Indeed, if we suppose for example that the components are Weibull distributed and we have interval on the shape and scale parameters it is possible that we have exact reliability function that cross and so upper and lower bound are not define by the lower and upper bound of the parameters. We need to estimates by for example Monte-Carlo methods the upper and lower bound. The procedure to implement is the following :

\begin{enumerate}[(1)]
	\item If there is imprecision in $\theta_k$, sampling the parameters from uniform distribution between the bounds.
	\item Sampling the transition time $t_k \in R^{m_k}$ of each type of components $k\  = 1,...,K$ from the distribution $F_k(t|\theta_k)$.
	\item Reordering the all the transitions time $(t_1,...,t_K)$ to have a vector of components failure time $t = (t_{(1)},...,t_{(m)})$ such that $t_{(i)} \leq t_{(i+1)}$. 
	\item Compute the reliability $\phi_i$ that the system function for each interval $[t_{(i)},t_{(i+1)})$, this gives a vector $(\phi_1,...,\phi_m)$ representing the reliability.
	\item Finally repeat $N$ times the step (2)-(4) and averaging it to get an estimation of the reliability function of the system.  
\end{enumerate}

Figure \ref{fig:boundsparam} shows an example of 5 reliability estimations of the system illustrated in the Figure \ref{fig:system_example} with two case of components distribution. For the case A illustrated in the Figure  \ref{fig:boundsparam0}, imprecisions is on the shape parameters while for the case B Figure  \ref{fig:boundsparam1} imprecision is on the scale parameters. Furthermore, the Figures contained also 5 realizations of the algorithm with $N$ equals to 1000 and the parameters are randomly chosen in the respective interval from uniform distribution. 


\begin{figure}[H]
 \centering
	\subfloat[Component 1 : shape $\thicksim\mathcal{U}(2.5,5)$ Component 2 :  shape $\thicksim\mathcal{U}(5,7.5)$. Parameter scale equals 50 for both component 1 and 2.]{\label{fig:boundsparam0}
	<<boundsparam0,echo=FALSE,fig.height=3.5,warning=FALSE,message=FALSE,results='hide',cache = TRUE>>=
		g <- graph.formula(s-1-2-t,s-1-3-t,s-2-3-4-t)
		g <- setCompTypes(g,list("T1" = c(1,2),"T2"=c(3,4)))
		survsig <- computeSystemSurvivalSignature(g)
	
		##############################################################
		F_t <- list(pweibull,pweibull)
			
		shapeI <- c(2.5,5)
		shapeII <- c(5,7.5)
		scaleI <- c(50,50)
		scaleII <- c(50,50)
		paramlow <- list(list(shape = 2.5,scale = 50),
		list(shape = 5,scale = 80))
		paramup <- list(list(shape = 5,scale = 50),
		list(shape = 7.5,scale = 50))
			
		t <- seq(0,110,0.1)
		rlow <- Relib(survsig,t,F_t,paramlow)
		rup <- Relib(survsig,t,F_t,paramup)
			
		######################################################
			
		F_k <- list(rweibull,rweibull)
			
		param <- list(list(shape = shapeI,scale = scaleI),list(shape= shapeII,scale = scaleII))
			
		datup <- data.frame(x = t,y = rup,Label = rep("BoundUp",length(t)),type = rep("Up",length(t)))
		datlow <- data.frame(x = t,y= rlow,Label = rep("BoundLow",length(t)),type = rep("Low",length(t)))
			
			
		p_tmp <- data.frame()
		for(j in 1:5){
			x = SamplingReliability(survsig,F_k,param)$t
			p_tmp <- rbind(p_tmp,data.frame(x = x,y =  SamplingReliability(survsig,F_k,param)$value,
			type = rep(paste("real",j),length(x)),Label = rep("Sample",length(x))))
		}
			
		cols <- c("BoundUp" = "brown4","BoundLow" = "aquamarine4", "Sample" = "lightskyblue4")
			
			
		ggplot() +
			geom_step(data = p_tmp,aes(x = x,y = y,group = type,color = Label))  +
			geom_line(data = datup,aes(x = x,y = y,group = type ,color = Label)) +
			geom_line(data = datlow,aes(x = x,y = y,group = type ,color = Label)) +
			scale_colour_manual(values = cols) +
			ggtitle("Bounds on component parameters : Case A") + 
			xlab("Failure times") + ylab("Reliability") +
			theme_light() +  theme(plot.title = element_text(hjust = 0.5))

		@
		}\\
\subfloat[Component 1 : scale $\thicksim\mathcal{U}(40,60)$ Component 2 :  scale $\thicksim\mathcal{U}(80,120)$. Parameter shape equals 2.5 for both component 1 and 2.]{\label{fig:boundsparam1}
	<<boundsparam1,echo=FALSE,fig.height=3.5,warning=FALSE,message=FALSE,results='hide',cache = TRUE>>=
 g <- graph.formula(s-1-2-t,s-1-3-t,s-2-3-4-t)
 g <- setCompTypes(g,list("T1" = c(1,2),"T2"=c(3,4)))
 survsig <- computeSystemSurvivalSignature(g)
 
 ##############################################################
 F_t <- list(pweibull,pweibull)
 
 shapeI <- c(2.5,2.5)
 shapeII <- c(5,5)
 scaleI <- c(40,60)
 scaleII <- c(80,120)
 paramlow <- list(list(shape = 2.5,scale = 40),
 list(shape = 5,scale = 80))
 paramup <- list(list(shape = 2.5,scale = 60),
 list(shape = 5,scale = 120))
 
 t <- seq(0,110,0.1)
 rlow <- Relib(survsig,t,F_t,paramlow)
 rup <- Relib(survsig,t,F_t,paramup)
 
 ######################################################
 
 F_k <- list(rweibull,rweibull)
 
 param <- list(list(shape = shapeI,scale = scaleI),list(shape= shapeII,scale = scaleII))
  
 datup <- data.frame(x = t,y = rup,Label = rep("BoundUp",length(t)),type = rep("Up",length(t)))
 datlow <- data.frame(x = t,y= rlow,Label = rep("BoundLow",length(t)),type = rep("Low",length(t)))
 
 
 
 
 p_tmp <- data.frame()
 for(j in 1:5){
 x = SamplingReliability(survsig,F_k,param)$t
 p_tmp <- rbind(p_tmp,data.frame(x = x,y =  SamplingReliability(survsig,F_k,param)$value,
 type = rep(paste("real",j),length(x)),Label = rep("Sample",length(x))))
 }
 
 cols <- c("BoundUp" = "brown4","BoundLow" = "aquamarine4", "Sample" = "lightskyblue4")
 
 
 ggplot() +
 geom_step(data = p_tmp,aes(x = x,y = y,group = type,color = Label))  +
 geom_line(data = datup,aes(x = x,y = y,group = type ,color = Label)) +
 geom_line(data = datlow,aes(x = x,y = y,group = type ,color = Label)) +
 scale_colour_manual(values = cols) +
 ggtitle("Bounds on component parameters : Case B") + 
 xlab("Failure times") + ylab("Reliability") +
 theme_light() +  theme(plot.title = element_text(hjust = 0.5))

@
}
\caption{Reliability function of system of order 4 illustrated in Figure \ref{fig:system_example} with the survival signature in Table \ref{tab:system_example_bound}. The failure time is sampled from Weibull distribution. }
\label{fig:boundsparam}
\end{figure}


Another quantity that might be useful to analyze is the relative importance of the components. The idea is to look at the component that are critical from a reliability point of view. Relative importance index ($RI$) is define by \cite{FENG2016116} as the difference between the probability that the system functions if the $i$th component works and the probability that the system functions if the $i$th component is not working. Formally we obtain :

\begin{equation}
RI_i(t) = \pr(T_{sys} > t| T_i > t) - \pr(T_{sys} > t| T_i \leq t)
\label{eq:relative_importance}
\end{equation} 
for each time we can then analyze the importance of every components and then understand the influence of the components on the reliability. Figure \ref{fig:RELATIVEIMPORTANCE} shows the relative importance of the 4 components of the system \ref{fig:system_example} with the survival signature in Table \ref{tab:system_example_bound}. The components are supposed to follow Weibull distribution with shape parameter 2.5 and scale parameter 50 for the type 1, and shape 5, and scale 100 for the type 2. Since the failure time of the components 2 are in average greater, we observe that the relative importance of the components 2 are greater as the time increase.


\begin{figure}[H]
	\centering
<<RELATIVEIMPORTANCE,echo=FALSE,fig.height=5,warning=FALSE,message=FALSE,results='hide',cache = TRUE>>=
rm(list = ls())

g <- graph.formula(s-1-2-t,s-1-3-t,s-2-3-4-t)
g <- setCompTypes(g,list("T1" = c(1,2),"T2"=c(3,4)))
survsig <- computeSystemSurvivalSignature(g)


# g1 is when the component is functionning so keep all relation
# g2 is when the component is failed so remove all relation


## g4
g1_4<- graph.formula(s-1-2-t,s-1-3-t,s-2-3-t)
g1_4 <- setCompTypes(g1_4,list("T1" = c(1,2),"T2"=c(3)))
survsig1_4 <- computeSystemSurvivalSignature(g1_4)

g2_4<- graph.formula(s-1-2-t,s-1-3-t)
g2_4 <- setCompTypes(g2_4,list("T1" = c(1,2),"T2"=c(3)))
survsig2_4 <- computeSystemSurvivalSignature(g2_4)

## g3
g1_3 <- graph.formula(s-1-2-t,s-1-t,s-2-4-t)
g1_3 <- setCompTypes(g1_3,list("T1" = c(1,2),"T2"=c(4)))
survsig1_3 <- computeSystemSurvivalSignature(g1_3)

g2_3 <- graph.formula(s-1-2-t,4-t)
g2_3 <- setCompTypes(g2_3,list("T1" = c(1,2),"T2"=c(4)))
survsig2_3 <- computeSystemSurvivalSignature(g2_3)

## g2
g1_2<- graph.formula(s-1-t,s-1-3-t,s-3-4-t)
g1_2 <- setCompTypes(g1_2,list("T1" = c(1),"T2"=c(3,4)))
survsig1_2 <- computeSystemSurvivalSignature(g1_2)

g2_2 <- graph.formula(s-1-3-t,4-t)
g2_2 <- setCompTypes(g2_2,list("T1" = c(1),"T2"=c(3,4)))
survsig2_2 <- computeSystemSurvivalSignature(g2_2)

## g1
g1_1 <- graph.formula(s-2-t,s-3-t,s-2-3-4-t)
g1_1 <- setCompTypes(g1_1,list("T1" = c(2),"T2"=c(3,4)))
survsig1_1 <- computeSystemSurvivalSignature(g1_1)

g2_1 <- graph.formula(s-2-3-4-t)
g2_1 <- setCompTypes(g2_1,list("T1" = c(2),"T2"=c(3,4)))
survsig2_1 <- computeSystemSurvivalSignature(g2_1)



F_t <- list(pweibull,pweibull)
paramF1 <- list(shape = 2.5,scale = 50)
paramF2 <- list(shape = 5,scale = 100)

param <- list(paramF1,
paramF2)
t <- seq(0,100,0.1)
ri_4 <- RI(survsig1_4,survsig2_4,t,F_t,param)
ri_3 <- RI(survsig1_3,survsig2_3,t,F_t,param)
ri_2 <- RI(survsig1_2,survsig2_2,t,F_t,param)
ri_1 <- RI(survsig1_1,survsig2_1,t,F_t,param)


cols <- c("Comp1" = "lightskyblue3", "Comp2" = "lightskyblue4","Comp3" = "ivory4","Comp4" = "rosybrown4")


dat <- data.frame(value = c(ri_1,ri_2,ri_3,ri_4),
x = c(t,t,t,t),
Label = c(rep("Comp1",length(ri_1)),rep("Comp2",length(ri_2)),rep("Comp3",length(ri_3)),rep("Comp4",length(ri_4))))

ggplot() + geom_line(data = dat,aes(x = x,y = value,color = Label)) +
scale_colour_manual(values = cols) +
ggtitle("Relative Importance of components") +
xlab("Failure times") + ylab("Relative importance") +
theme_light() +  theme(plot.title = element_text(hjust = 0.5))
	
@
\caption{Relative importance of the components of the system of order 4 illustrated in Figure \ref{fig:system_example} with the survival signature in Table \ref{tab:system_example_bound}.}
\label{fig:RELATIVEIMPORTANCE}
\end{figure}


%-----------------------------------------------------------------------------------
%	BAYESIAN P. I.
%-----------------------------------------------------------------------------------

%\subsection{Bayesian predictive inference}

%-----------------------------------------------------------------------------------
%	MONTE-CARLO
%-----------------------------------------------------------------------------------

\subsection{Monte-Carlo approach and repairable components}

We have seen how the reliability can be approach when only partial informations is available. Here we will introduce another way to put some imprecision on the model.\\
 \cite{PatelliSimulation} present a Monte-Carlo algorithm similar with the one developed previously but with the possibility to incorporate some state on the components. Hence this is not anymore failure time that we have to sampled but transition time. Indeed, components could pass from a state to another. Formally, let $s_1$ and $s_2$ be two possible state of the component of type $k$. Then the probability that the component going from the sate $s_1$ to the state $s_2$ is given by $p_{s_1\to s_2}^k = \pr(X_k = s_2 | X_k = s_1)$ where $X_k$ is the random variable that represent the state of the components. We could then write the CDF of the components of type $k$ to exit from the state $s_j$ as 
 $$ F_{s_j}^k = \sum_{i \neq j}  \pr(X_k = s_i | X_k = s_j)$$
 
 The main problems that arise with this point of view is that there is no analytical solutions available for systems with multiple state \cite{PatelliSimulation}, but one might estimate the reliability with Monte-Carlo simulations. The procedure is as follows:
 
\begin{enumerate}[(1)]
	\item Initialize constant value such as $C$ the total number of components, $K$ the number of type of components, $V_c = (C_1,...,C_k)$, $V_s = (s_1,...,s_K)$ where $s_i$ is the state of the $i$th components. $t_{old} = 0$ and $Vr$ that will sum the sampled reliability.
	\item Sample the transition times $t_i$ for $i = 1,2,...,C$ from the right CDF $F_{s_j}^k$ depending on the state of each components $V_s$. Keep all this transition time in a vector $V_t$.
	\item Find the the minimum transition time $t_z$ and its components index $z$.
	\item For all discretization time that are in the interval $[t_{old},t_z]$.  Calculate the survival signature of the number of components that are in working status. i.e. for all $j$ such that $j\times dt \in [t_{old},t_z]$ do $Vr(j)  = Vr(j)+  \phi(V_c)$.
	\item Set $t_{old} = t_z$ and sample the new status of the components $z$. (i.e. change the status of $z$ with the right state).
	\item Update the transition time vector $V_t$ by sampling the next transition time $t_z^{new}$ of the component z with the right status previously changed. Then $V_t(z) = t_z + t_z^{new}$.
	\item If $min(V_t)$ is lower than a final time $T_F$ then return to (3).
	\item Finally repeat $N$ times the step (1)-(7) and averaging it to get an estimation of the reliability function of the system.

\end{enumerate}

The algorithm presented by \cite{PatelliSimulation} is principally used to incorporate reparable components. Here we will used to put some state on components and follow the idea that all the components begins in the same state and should pass through all state until inactivity. Moreover, we will incorporate a probability of back to a previous state if we reach inactivity. 



\begin{figure}[!h]
	\centering
	\subfloat[States represnetation of each components. This means that each component begins at the states 3 and follows different distribution between the states.]{\label{fig:3statespattern}\includegraphics[width=0.9\textwidth]{diagramstep}}\\
	\subfloat[Reliability based on the state pattern \ref{fig:3statespattern}.  ]{\label{fig:bafjbadf}
<<StateComponents1,echo=FALSE,fig.height=3.5,warning=FALSE,message=FALSE,results='hide',cache = TRUE>>=
	g <- graph.formula(s-1-2-3-t,s-1-4-6-t,4-5-6,2-5-3)
	g <- setCompTypes(g,list("T1" = c(1,2,3),"T2"= c(4,5,6)))
	survsig <- computeSystemSurvivalSignature(g)
	
	V_c <- c(3,3)
	N <- 1000
	Nt <- 200
	d_t <- 0.5
	
	paramInput3 <- list(list(n = 1,shape = 2,scale = 25),
	list(n = 1,shape = 2,scale = 25))
	paramInput2 <- list(list(n = 1,min = 25,max = 35),
	list(n = 1,min = 25,max = 35))
	paramInput1 <- list(list(n = 1,shape = 3,scale = 10),
	list(n = 1,shape = 3,scale = 10))
	paramInput0 <- list(list(n = 1,rate = 0.8*8),
	list(n = 1,rate = 1.5*8))
	
	F_kInput3 <- list(rweibull,rweibull)
	F_kInput2 <- list(runif,runif)
	F_kInput1 <- list(rgamma,rgamma)
	F_kInput0 <- list(rexp,rexp)
	
	Simul2 <- Algo4(survsig,N,d_t,Nt,V_c, F_kInput3,
	F_kInput2,
	F_kInput1,
	F_kInput0,
	paramInput3,
	paramInput2,
	paramInput1,
	paramInput0)
	
	
	ggplot(data = data.frame(Time = Simul2$grid,Survival = Simul2$value)) +
	geom_line(aes(x = Time, y = Survival),colour =  "lightskyblue4") +
	ggtitle("3-States components reparaible reliability") +
	xlab("Time") +
	theme_light() +  theme(plot.title = element_text(hjust = 0.5))
	@
}
\caption{Diagram procedure example for the incorporation of state of components. The state could be in four state each state coul}
\label{fig:diagramstep}
\end{figure}






%-----------------------------------------------------------------------------------
%	FRAILTY MODEL
%-----------------------------------------------------------------------------------

%\section{Frailty Models}
%
%\begin{prop}
%	Let $T|\lambda$ be a Weibull random variable, such that its survival function is 
%	$$ S(t|\lambda) = e^{-\lambda t^m}. $$
%	If we suppose that $\lambda\in \Rplus$ a random variable with density function $f_\lambda$ , then we have that 
%	$$ S(t) = \mathcal{L}(f(t^m)) $$
%\end{prop}
%\begin{proof}
%	$$\begin{array}{lll}
%		S(t) &=&\D \int_{\Rplus} S(t|\lambda) dF_\lambda\\
%		&=&\D \int_{\Rplus} S(t|\lambda) f_\lambda(\lambda)d\lambda\\
%		& = &\D \int_{\Rplus} e^{-\lambda t^m} f_\lambda(\lambda) d\lambda\\
%		& = & \D \mathcal{L}(f(t^m))
%		
%	\end{array}$$
%\end{proof}
%
%
%\begin{prop}
%	Let $ T | \lambda$ be a Weibull random variable, such that its repartition function is
%	$$ F_{T|\lambda}(t) = 1- e^{-\lambda t^m} $$
%	 if $\lambda$ follow a Gamma distribution with shape $\alpha$ and rate $\beta$ then the repartition function of the marginal variable X is,
%	$$ 1 - (1 + \frac{1}{\beta}t^m)^{-\alpha} $$
%\end{prop}
%\begin{proof}
% In order to find the marginal distribution of the variable $T$ we need to integrate the joint probability on the support of $\lambda$. This gives us,
%$$\begin{array}{lll}
% f_T(t) & = &\D \intI f(t|\lambda) dF_\lambda\\
% & = & \D\intI m\lambda t^{m-1} e^{-\lambda t^m} \frac{\lambda^{\alpha - 1} \beta^\alpha e^{-\beta\lambda}}{\Gamma(\alpha)}d\lambda\\
% & = &\D \frac{mt^{m-1}\beta^\alpha}{\Gamma(\alpha)}\intI \lambda^\alpha e^{-(t^m + \beta)\lambda} d\lambda\\
% & = & \D \frac{mt^{m-1}\beta^\alpha}{\Gamma(\alpha)(t^m + \beta)^\alpha }\intI ((t^m + \beta)\lambda)^\alpha e^{-(t^m + \beta)\lambda}d\lambda\\
% & = & \D \frac{mt^{m-1}\beta^\alpha}{\Gamma(\alpha)(t^m + \beta)^{\alpha +1}}\intI \nu^\alpha e^{-\nu} d\nu\\
% & = & \D \frac{mt^{m-1}\beta^\alpha \Gamma(\alpha + 1)}{(t^m + \beta)^{\alpha +1}\Gamma(\alpha)}\\
% & = & \D \frac{m\alpha\beta^\alpha t^{m-1}}{(t^m + \beta)^{\alpha +1}}\\
%\end{array}
% $$
%To find the repartition function we need to integrate the density function over, we obtain then,
%$$\begin{array}{lll}
%F_T(t) & = &\D \int_0^t \frac{m\alpha\beta^\alpha \nu^{m-1}}{(\beta + t^m)^{\alpha + 1}} d\nu \\
%& = &\D \alpha\beta^\alpha \int_\beta^{\beta + t^m} s^{-\alpha-1} ds \\
%& = &\D 1 - (1 + \frac{1}{\beta}t^m)^{-\alpha}
%\end{array}
%$$
%
%\end{proof}
%blab
%<<test,echo=TRUE,fig.pos='!h',fig.align='center',fig.height=4,warning=FALSE,message=FALSE,fig.cap=' ',results='hide'>>=
%# lambda <- 0.00001 # 0.01 =  1/100
%# m = 2
%# t <- seq(0,100,length.out = 300)
%# a <- lambda^m*1000
%# b <- 1000
%# S1 <- hazBurr(t,a = a,b =b,m = m)
%# S2 <- hazWeib(t,m,lambda)
%#
%# dat <- data.frame(Burr = S1,Fixed = S2)
%# dat <- reshape2::melt(dat)
%# colnames(dat) <- c("Type","Hazard")
%# dat$Time <- rep(t,2)
%# ggplot(dat) + geom_line(aes(x = Time,y = Hazard,color = Type)) + theme_light()
%
%@


%-----------------------------------------------------------------------------------
%	Carcinogeneis model
%-----------------------------------------------------------------------------------
\clearpage
\section{Carcinogenesis model}
Cancer is one of the principal cause of mortality in many developed country, lung and colon are the most prevalent. In the 1950s Armitage and Doll developed the idea that cancer might be a process of cellular mutation. They worked on the idea that cancer is created by successive mutation in cells. Their hypothesis was based on previous study of Nordling on cancer mortality rate \cite{Gsteiger2006}. These models had not yet supposed the notion of initiated or pre-neoplasic cells. They are generally called \textit{multi-hit} models.\\

In 1979 Moolgavkar and Venzen and Moolgavkar and Knudson in 1981 for it enhanced version, developed the \textit{two-stage} models. The basic idea is that progression of healthy cells to neoplasic cells is done in two-stage. The first one is called initiation, the cells acquired some properties that are not classical such as growth advantage. The second stage is the promotion, initiated cells follows a branching (counting) process with birth and death rate with neoplasic apparition. The models is generally called Moolgavkar-Venzen-Knudson (MVK) models. Hence since 1950 there are amelioration of the models and some other concept that used stochastic differential equation and microenvironment. To get a good overview of the different models on might consult \cite{Vineis2010}. \\

We will present an brief overview of the \textit{multi-hit} model and the \textit{two-stage} models. We will then present some models that incorporate heterogeneity in the model. Indeed, it seems natural to think that the biological parameters of the models is not exactly the same for everyone or some environmental, epigenetic mechanisms induced heterogeneity. To have an good overview of the models we refer to \cite{Morgenthaler} and for heterogeneity on could look at \cite{Gsteiger2006}.


\subsection{Multi-hit model}
Multi-hit model supposed a number of mutation on the cell to transform a healthy cells into a neoplasic one. We will supposed an organ form of $N$ cells. Let $T_1,...,T_N$ be the survival time of each cells $1,2,...,N$. If we supposed that the necessary hit $n = 1$ then the survival time $T$ of the organ is given by 
\begin{equation}
T = \min(T_1,...,T_N).
\label{eq:min_time}
\end{equation}
Indeed the first apparition of neoplasic cells in the organ suffice to developed cancer. If we then supposed that the survival time $T_1,...T_N$ are exponentially distributed such that $T_i \thicksim \mathcal{E}(\nu)$ are independent for all $i = 1,...,N$ and $\nu > 0$ represent the rate of mutation apparition. Then we obtain that the survival time of the organ is given by
\begin{eqnarray}
S(t) &=& \pr(T > t) \nonumber\\
&=& \pr(T_1 >t,...,T_N > t) \nonumber\\
& =& \pr(T_1 > t)^N \nonumber\\
&=& e^{N \nu t}
\end{eqnarray} 
We then observe that in this case the survival time is distributed as $T \thicksim \mathcal{E}(N\nu)$. To generalize for multi-hit we could then regard each survival time $T_i$ and supposed for example that two allele must be inactivated to reach neoplasic state. Formally this supposed that

\begin{equation}
T_i = max(T_i^a,T_i^b).
\label{eq:max_time}
\end{equation}
If we suppose that the two allele are exponentially distributed and independant, we obtain that the cumulative distribution of the survival time $T_i$ is 
 \begin{eqnarray}
 F_i(t) &=& \pr(T_i \leq t) \nonumber\\
 &=& \pr(T_i^a \leq t,T_i^b \leq b) \nonumber\\
 & =& \pr(T_i^a \leq t)\pr(T_i^b\leq t) \nonumber\\
 &=& (1-e^{-\nu t})^2. 
 \end{eqnarray}

We obtain the survial function $S_i(t) = 1-F_i(t) = 1-(1-e^{\nu t})^2 = e^{\nu t}(2- e^{-\nu t})$. If we suppose that the rate $\nu$ is near of 0 we could then approximate the exponential quantity 

\begin{equation}
 e^{-\nu t} = 1 - \nu t + \frac{\nu^2t^2}{2} + o(\nu^2 t^2)
 \label{eq:approximation}
\end{equation}
This gives that for each survival time $T_i$ we could approximate it by 
\begin{equation}
S_i(t) = \left(1-\nu t + \frac{\nu^2 t^2}{2}\right)e^{-\nu t}
\end{equation}
The survival time of the whole organ is as previously the minimum of all variable. Hence we obtain,
\begin{eqnarray}
S(t) &=& (S_i(t))^N \nonumber\\
&=& e^{-N\nu t}\left(1 + \nu t- \frac{\nu^2 t^2}{2}\right)^N \nonumber\\
&=& e^{-N\nu t + N \log\left(1 + \nu t - \frac{\nu^2 t^2}{2}\right)} \nonumber\\
&=& e^{-N\nu t}\times e^{N\nu t - \frac{N\nu^2 t^2}{2} - \frac{N \nu^2 t^2}{2}}\nonumber\\
&=& e^{-N \nu^2 t^2}
\end{eqnarray}  

In order to generalize this result for a fixed number of step $n$. We would do it with a differential equation approach it gives an easier comprehension than just do it with more approximation step. It is based on the multi-hit model in continuous time in \cite{Morgenthaler}.

\begin{prop}
	Let an organ with $N$ cell, if we suppose that $n$ step are needed in the multi-hit carcinogenesis model such that each step could be appears in any order. Then the survival and hazard function can be approximated by the formulas :
	\begin{equation}
	S(t) = e^{-N\nu^n t^n}
	\label{eq:surv_MulitHit}
	\end{equation}
	\begin{equation}
	h(t) = -\frac{d}{dt}\log(S(t)) = n\nu^nNt^{n-1}
	\label{eq:hazard_MulitHit}
	\end{equation}
\end{prop}
\begin{proof}
Supposed first that the	order of the $n$ mutation are in a given order $1\to 2 \to ...\to n$. Let $I_j(t)$ be the number of cell that are mutated $j$ times in the organ. Moreover let $M_j(t) = \E[I_j(t)]$ be the expected number of $j$ times mutated cells. If we let $dt > 0$ then we could expressed the survival function by conditional and product probability :
$$ S(t + dt ) = S(t)(1-M_{n-1}(t)\nu dt + o(dt))$$ 
where $S(t)$ is the probability that cells has survived until $t$ multiply by the probability that there were no mutated cells in the interval $[t,t+dt]$ by rewriting it we obtain then
$$ -\frac{S(t+dt)-S(t)}{S(t)dt} = M_{n-1}(t)\nu + \frac{o(dt)}{dt}$$
by taking the limit when $dt \to 0$ we obtain that the survival function is written by
$$ S(t) = \exp\left(-\nu \int_0^t M_{n-1}(u)du\right)$$ 
In order to find a recurrence function between the variable $M_j$ and $M_{j-1}$, we could observe that $I_j(t + dt) = I_j(t)$ or $I_j(t+dt) = I_j(t) + 1$.
Hence the variable $M_j(t + dt)$ could be written by 
\begin{eqnarray}
 M_j(t + dt) &=& \E[I_j(t + dt)] \nonumber\\ 
 &=& \E[\E[(\nu I_{j-1}(t)dt)(I_j(t)+ 1) + (1-\nu I_j(t)dt)(I_j(t)) + o(dt))]] \nonumber\\
 &=& \nu M_{j-1}(t) dt + M_j(t) + o(dt)\nonumber
\end{eqnarray}
As previously we could then rewrite the expected number of $j$ times mutated cells by :
$$ M_j(t) = \int_0^t M_{j-1}(u)du $$
and by the fact that $M_0(t) = N$ we obtain then that $M_j(t) = \frac{\nu^nNt^{n-1}}{(n-1)!}$ and the survival and hazard function are given by :
$$ S(t) = \exp\left(-\frac{\nu^n N t^n}{n!}\right) \text{ and } h(t) = \frac{\nu^nNt^{n-1}}{(n-1)!}$$
Now if we supposed that the number of mutation could appears in any order, we just have to multiply the hazard function by the number of possible order $n!$ and we obtain the desired result :
$$ S(t) = \exp\left(-N\nu^n t^n\right) \text{ and } h(t) = n N\nu^n t^{n-1}$$

\end{proof}

We see that waiting time follow a Weibull distribution with shape parameter equal to $n$ and scale parameter equal to $(\nu N^{\frac{1}{n}})^{-1}$. Another point of view is to see that the process that count the number of cells that have undergo $n$ mutation is a Poisson process with non-homogeneous rate equal to $\int_0^t h(u)du$ \cite{Gsteiger2006} . Figure \ref{fig:multihitmodel} shows three examples of multi-hit process   

\begin{figure}[H]
\centering
<<Multi_hit_model,echo=FALSE,fig.height=5,warning=FALSE,message=FALSE,results='hide',cache = TRUE>>=

rm(list = ls())


t <- seq(0,100,1)
N <- 10^10

nu7 <- 6.2*10^{-4}
n7 <- 7

nu4 <- 5.2*10^{-5}
n4 <- 4

nu2 <- 1.9*10^{-7}
n2 <- 2


datsurv7 <- data.frame(x = t,y = S_cancerMH(t,n7,nu7,N)$survival,Label = rep("n = 7",length(t)),type = rep("survival",length(t)))
datsurv4 <- data.frame(x = t,y = S_cancerMH(t,n4,nu4,N)$survival,Label = rep("n = 4",length(t)),type = rep("survival",length(t)))
datsurv2 <- data.frame(x = t,y = S_cancerMH(t,n2,nu2,N)$survival,Label = rep("n = 2",length(t)),type = rep("survival",length(t)))

dathaz7 <- data.frame(x = t,y = S_cancerMH(t,n7,nu7,N)$hazard,Label = rep("n = 7",length(t)),type = rep("hazard",length(t)))
dathaz4 <- data.frame(x = t,y = S_cancerMH(t,n4,nu4,N)$hazard,Label = rep("n = 4",length(t)),type = rep("hazard",length(t)))
dathaz2 <- data.frame(x = t,y = S_cancerMH(t,n2,nu2,N)$hazard,Label = rep("n = 2",length(t)),type = rep("hazard",length(t)))

cols <- c("n = 2" = "lightskyblue3", "n = 4" = "lightskyblue4","n = 7" = "ivory4")

dat <- rbind(datsurv2,datsurv4,datsurv7,dathaz2,dathaz4,dathaz7)

ggplot(data =dat ) + geom_line(aes(x = x,y = y,color = Label)) + facet_wrap(~type,scale = "free") +
scale_colour_manual(values = cols) +
ggtitle("Multi-hit model") +
xlab("Time") +
theme_light() +  theme(plot.title = element_text(hjust = 0.5))
@
\caption{Survival and hazard function example of the multi-hit models. The numbers of cells is fixed at $N = 10^{10}$ and the parameters are equal to $\nu_7 = 6.2\times 10^{-4}$, $\nu_4 = 5.2\times 10^{-5}$ and $\nu_2 = 1.9\times 10^{-7}$.}
\label{fig:multihitmodel}
\end{figure}

\subsection{Multi-hit model as system and reliability}

The multi-hit can be also viewed in terms of reliability and survival signature. Indeed, equations \eqref{eq:min_time} and \eqref{eq:max_time} suppose that the distribution of the multi-hit model is just the minimum time of observing a neoplasic cells that occurs at the maximum time of a number of allele. But this follow exactly the same idea developed in the survival and system signature. Hence, if we supposed a multi-hit model with $n = 2$ then the corresponding system and its survival signature is just the parallel system with 2 components with each components distributed as exponential with rate parameter equal $\nu$.\\

Though each 2-parallel system will represent the system's life of the cell and for the simple implementation of the multi-hit, the system is only characterize by the two allele that are possibly inactivated. Now as the organ contain $N$ cells there are two possible paradigm.\\

1) We modeling the whole organ by a system and in the case of multi-hit model we just put the all system in series. This might be a good approach since we could possibly incorporate some dependence between the different cells by changing the series hypothesis. But the biggest problem with this solution is that the number $N$ could be very huge and compute survival signature for system with more than hundred components become cumbersome.\\
 
2) We suppose that each survival time of the cells are independent. The multi-hit suppose independence as well and we can just put the reliability of one system'cells to the power of $N$ to get approximately the same survival function for the whole organ.\\


The survival function of the conception are not exactly the same. Indeed the multi-hit model use a limiting development \eqref{eq:approximation} this gives us that if the approximation is poor then we get a significant difference between the two survival function. Formally we get that the survival function where the approximation is made is 
\begin{equation}
S(t) = e^{-N\nu^nt^n}
\end{equation}
and the one based on the system reliability is 
\begin{equation}
S(t) = \left(1- (1-e^{-\nu t})^n\right)^N
\end{equation}
Figure \cite{fig:Comparaison_mulit_hit} shows a comparison between the 2-multi-hit model and the reliability estimate for a system with 2-parallel component with number of cells fixed for both model to $N = 5$ and rate parameter fixed at $\nu = 9.2 \times 10^{-3}$. We can see that as the time increase the difference begins bigger.

\begin{figure}[H]
\centering
<<ComparaisonMulit_hit,echo=FALSE,fig.height=5,warning=FALSE,message=FALSE,results='hide',cache = TRUE>>=

rm(list = ls())

t <- seq(0,100,1)
nu <- 9.2*10^{-3}
N <- 5
n = 2

# g <- graph.formula(s-1-3-5-7-9-t,
#                    s-2-4-6-8-10-t,
#                    1-4-5-8-9,
#                    2-3-6-7-10)
# 
# g <- setCompTypes(g,types = list("T1" = c(1,3,5,7,9),"T2"=c(2,4,6,8,10)))
# 
# g2 <- setCompTypes(g,types = list("T1" = c(1,3,5,7),"T2"=c(2,4,6,8),"T3"=c(9,10)))
# survsig <- computeSystemSurvivalSignature(g)
# survsig2 <- computeSystemSurvivalSignature(g2)
# 
# 
# F_t <- list(pexp,pexp)
# 
# paramF1 <- list(rate = nu)
# paramF2 <- list(rate = nu)
# 
# param <- list(paramF1,
#               paramF2)
# F_t2 <-list(pexp,pexp,pexp)
# 
# param2 <- list(paramF1,
#               paramF2,
#               paramF2)
# 
# r <- Relib(survsig,t,F_t,param)
# r2 <- Relib(survsig,t,F_t2,param2)
# lines(t,r,col = "red")
# lines(t,r2,col = "blue")

dat1 <- data.frame(x = t,y = S_cancerMH(t,n,nu,N)$survival,Label = rep("Approx",length(t)))
g3 <- graph.formula(s-1-t,s-2-t)
g3 <- setCompTypes(g3,types = list("T1" = c(1),"T2"=c(2)))
survsig3 <- computeSystemSurvivalSignature(g3)


F_t <- list(pexp,pexp)

paramF1 <- list(rate = nu)
paramF2 <- list(rate = nu)

param <- list(paramF1,
paramF2)
r <- Relib(survsig3,t,F_t,param)

dat2 <- data.frame(x = t,y = r^N,Label = rep("Exact",length(t)))


dat <- rbind(dat1,dat2)



cols <- c("Approx" = "lightskyblue3", "Exact" = "lightskyblue4")


ggplot() + geom_line(data = dat,aes(x = x,y = y,color = Label))+
scale_colour_manual(values = cols) +
ggtitle("Comparison between multi-hit and system based") +
xlab("Time") +
theme_light() +  theme(plot.title = element_text(hjust = 0.5))




@
\caption{Comparison between the 2-multi-hit model and the reliability estimate for a system with 2-parallel component with number of cells fixed for both model to $N = 5$ and rate parameter fixed at $\nu = 9.2 \times 10^{-3}$.  }
	\label{fig:Comparaison_mulit_hit}
\end{figure}

Hence one could see that the system based approach can be similar to the multi-hit model. Moreover it gives a completely new approach since we can change the system. The system here was just took to be exactly the same interpretation of the multi-hit model. Though we could imagine a new system that represent the evolution of a normal cell to a neoplasic one. Furthermore, the model could incorporate dependence between the cells or just be the multiplication of reliability in case of independence.
\subsection{Two-stage model}

\subsection{Mixture model}

\section{Approach similarities}




%-----------------------------------------------------------------------------------
%	Disscussion
%-----------------------------------------------------------------------------------

\section{Discussion}

%-----------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%-----------------------------------------------------------------------------------
\clearpage
\bibliographystyle{apalike}
\bibliography{C:/Users/Raphael/Documents/PDM/bibtex/library}


\end{document}
